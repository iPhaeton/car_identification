{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import img_size\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Flatten, BatchNormalization, subtract\n",
    "from keras.models import Model\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.regularizers import l2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seamese_Model:\n",
    "    def __init__(self, base_model, opts, kernel_opts=None, bias_opts=None):\n",
    "        self.base_model = base_model\n",
    "        self.opts = opts\n",
    "        self.kernel_opts = kernel_opts\n",
    "        self.bias_opts = bias_opts\n",
    "    \n",
    "    def kernel_initializer(self, shape, name=None):\n",
    "        if (self.kernel_opts != None):\n",
    "            values = np.random.normal(loc=self.kernel_opts['loc'], scale=self.kernel_opts['scale'], size=shape)\n",
    "            return K.variable(values, name=name)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def bias_initializer(self, shape, name=None):\n",
    "        if (self.bias_opts != None):\n",
    "            values = np.random.normal(loc=self.bias_opts['loc'], scale=self.bias_opts['scale'], size=shape)\n",
    "            return K.variable(values, name=name)\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def get_top_model(self):\n",
    "        input_1 = Input(self.opts['features_shape'])\n",
    "        input_2 = Input(self.opts['features_shape'])\n",
    "        \n",
    "        X_1 = Flatten()(input_1)\n",
    "        X_2 = Flatten()(input_2)\n",
    "        X = subtract([X_1, X_2])\n",
    "        X = Dense(self.opts['features_shape'][1], activation='relu', name='dense_0', kernel_regularizer=l2(1e-3))(X)\n",
    "        X = BatchNormalization()(X)\n",
    "        output = Dense(1, activation='sigmoid', name='log_reg', kernel_regularizer=l2(1e-3))(X)\n",
    "        \n",
    "        model = Model(input=[input_1, input_2], output=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Supervisor:\n",
    "    # use seed only for testing\n",
    "    def __init__(self, model, data_train, classes_train, filenames_train, data_dev, classes_dev, filenames_dev, seed=None):\n",
    "        self.model = model\n",
    "        self.data_train = data_train\n",
    "        self.classes_train = classes_train\n",
    "        self.filenames_train = filenames_train\n",
    "        self.data_dev = data_dev\n",
    "        self.classes_dev = classes_dev\n",
    "        self.filenames_dev = filenames_dev\n",
    "        self.seed = seed\n",
    "        \n",
    "    def get_pair(self, index_1, index_2):\n",
    "        el_1 = np.take(self.data_train, [index_1], axis=0)\n",
    "        el_2 = np.take(self.data_train, [index_2], axis=0)\n",
    "        return([el_1, el_2])\n",
    "    \n",
    "    def get_selection_index(self, index, indices):\n",
    "        selection_index = index\n",
    "        while selection_index == index:\n",
    "            selection_index = np.random.choice(indices, 1)[0]\n",
    "        return selection_index\n",
    "    \n",
    "    def get_batch(self, n, data, classes):\n",
    "        np.random.seed(self.seed)\n",
    "        indices = np.random.choice(list(range(len(data))), size=n)\n",
    "        pairs = []\n",
    "        y = []\n",
    "        \n",
    "        for index in indices[:n//2]:\n",
    "            selection_indices = np.argwhere(classes == classes[index]).flatten()\n",
    "            selection_index = self.get_selection_index(index, selection_indices)\n",
    "            pairs.append(self.get_pair(index, selection_index))\n",
    "            y.append(1)\n",
    "            \n",
    "        for index in indices[n//2:]:\n",
    "            selection_indices = np.argwhere(classes != classes[index]).flatten()\n",
    "            selection_index = self.get_selection_index(index, selection_indices)\n",
    "            pairs.append(self.get_pair(index, selection_index))\n",
    "            y.append(0)\n",
    "            \n",
    "        return (np.array(pairs), np.array(y))\n",
    "    \n",
    "    def train(self, iterations, batch_size, validation_size=0, validate_every=float('inf'), learning_rate=0.0001, path=None, k=[1]):\n",
    "        self.model.compile(optimizer=tf.train.AdamOptimizer(learning_rate),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "        \n",
    "        if path != None:\n",
    "            self.model.load_weights(path)\n",
    "            \n",
    "        for i in tqdm_notebook(range(iterations)):\n",
    "            inputs, targets = self.get_batch(batch_size, self.data_train, self.classes_train)\n",
    "            metrics = self.model.train_on_batch([inputs[:,0,:], inputs[:,1,:]], targets)\n",
    "            if (i % validate_every == 0) & (i != 0):\n",
    "                self.checkpoint(batch_size, validation_size, i, metrics, k)\n",
    "    \n",
    "    def get_validation_task(self, data, classes, filenames, show_filenames=False, index = None):\n",
    "        if index == None:\n",
    "            index = np.random.choice(range(data.shape[0]), 1)[0]\n",
    "        \n",
    "        targets = np.repeat(np.take(data, [index], axis=0), data.shape[0] - 1, axis=0)\n",
    "        support_set = np.delete(data, index, axis=0)\n",
    "        pairs = np.stack([targets, support_set], axis=1)\n",
    "        \n",
    "        pair_filenames = None\n",
    "        pair_classes = None\n",
    "        target_class = None\n",
    "        support_classes = None\n",
    "        if show_filenames == True:\n",
    "            target_filenames = np.repeat(np.take(filenames, [index], axis=0), data.shape[0] - 1, axis=0)\n",
    "            target_class = np.take(classes, [index], axis=0)\n",
    "            support_filenames = np.delete(filenames, index, axis=0)\n",
    "            support_classes = np.delete(classes, index, axis=0)\n",
    "            pair_filenames = np.stack([target_filenames, support_filenames], axis=1)\n",
    "        \n",
    "        target_y = classes[index]\n",
    "        y = (np.delete(classes, index) - target_y) == 0\n",
    "\n",
    "        return pairs.reshape(pairs.shape[0], pairs.shape[1], 1, pairs.shape[2]), y, pair_filenames, target_class, support_classes\n",
    "    \n",
    "    def calculate_accuracy(self, n, batch_size, data, classes, filenames, ks, verbose=False, show_filenames=False):\n",
    "        incorrect_answers_count = np.zeros(len(ks))\n",
    "        incorrect_filenames = np.zeros(len(ks))\n",
    "        \n",
    "        for i in tqdm_notebook(range(n)):\n",
    "            inputs, targets, pair_filenames, target_class, support_classes = self.get_validation_task(data, classes, filenames, show_filenames, i)\n",
    "            predictions = self.model.predict([inputs[:,0,:], inputs[:,1,:]], batch_size=batch_size)\n",
    "            \n",
    "            for ind, k in enumerate(ks):\n",
    "                probs = np.flipud(np.sort(predictions.flatten()))[0:k]\n",
    "                indices = np.flipud(np.argsort(predictions.flatten()))[0:k]\n",
    "                pred_classes = np.take(support_classes, indices, axis=0)\n",
    "                unique_classes = np.unique(pred_classes)\n",
    "\n",
    "                class_to_index = {}\n",
    "                for i, cl in enumerate(unique_classes):\n",
    "                    class_to_index[cl] = i\n",
    "\n",
    "                counts = np.zeros(len(unique_classes))\n",
    "                sum_probs = np.zeros(len(unique_classes))\n",
    "                for cl, prob in zip(pred_classes, probs):\n",
    "                    counts[class_to_index[cl]] += 1\n",
    "                    sum_probs[class_to_index[cl]] += prob\n",
    "\n",
    "                pred_class = None\n",
    "                max_count = 0\n",
    "                max_sum_prob = 0\n",
    "                for cl, count, sum_prob in zip(unique_classes, counts, sum_probs):\n",
    "                    if count > max_count:\n",
    "                        pred_class = cl\n",
    "                        max_count = count\n",
    "                        max_sum_prob = sum_prob\n",
    "                    elif (count == max_count) & (sum_prob > max_sum_prob):\n",
    "                        pred_class = cl\n",
    "                        max_count = count\n",
    "                        max_sum_prob = sum_prob\n",
    "\n",
    "                if pred_class != target_class[0]:\n",
    "                    incorrect_answers_count[ind] += 1\n",
    "#                if show_filenames == True:\n",
    "#                    incorrect_filenames.append(pair_filenames[index])\n",
    "                \n",
    "        return list(map(lambda x: 1 - x/n, incorrect_answers_count))\n",
    "    \n",
    "    def checkpoint(self, batch_size, validation_size, iteration, metrics, ks=[1]):\n",
    "#        train_acc, incorrect_filenames_train = self.calculate_accuracy(\n",
    "#            n=validation_size, \n",
    "#            batch_size=batch_size, \n",
    "#            data=self.data_train, \n",
    "#            classes=self.classes_train, \n",
    "#            filenames=self.filenames_train, \n",
    "#            ks=ks,\n",
    "#        )\n",
    "        dev_acc = self.calculate_accuracy(\n",
    "            n=validation_size, \n",
    "            batch_size=batch_size, \n",
    "            data=self.data_dev, \n",
    "            classes=self.classes_dev, \n",
    "            filenames=self.filenames_dev, \n",
    "            ks=ks,\n",
    "            show_filenames=True,\n",
    "        )\n",
    "        print(dev_acc)\n",
    "        print('Iteration ' + str(iteration) + '. Batch metrics [loss, accuracy]:', metrics)\n",
    "        for i, k in enumerate(ks):\n",
    "            print('Iteration ' + str(iteration) + '. Validation accuracy (k=' + str(k) + '):', dev_acc[i])\n",
    "        print('---------')\n",
    "        self.model.save_weights('../input/siamese_model_weights/weights-' + str(iteration) + '-' + str(round(dev_acc[0], 4)) + '.hdf5')\n",
    "        \n",
    "    def validate(self, weights, data, classes, filenames, n=350, batch_size=128, ks=[5], verbose=False):\n",
    "        self.model.load_weights(weights)\n",
    "        \n",
    "        acc = self.calculate_accuracy(\n",
    "            n=n, \n",
    "            batch_size=batch_size, \n",
    "            data=data, \n",
    "            classes=classes, \n",
    "            filenames=filenames, \n",
    "            ks=ks,\n",
    "            show_filenames=True,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        \n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = np.load('../input/features/siamese/features_train.npy')\n",
    "features_dev = np.load('../input/features/siamese/features_dev.npy')\n",
    "classes_train = np.load('../input/features/siamese/classes_train.npy')\n",
    "classes_dev = np.load('../input/features/siamese/classes_dev.npy')\n",
    "filenames_train = np.load('../input/features/siamese/filenames_train.npy')\n",
    "filenames_dev = np.load('../input/features/siamese/filenames_dev.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seamese_Model(\n",
    "    ResNet50, \n",
    "    opts={\n",
    "        'features_shape': (1, features_train.shape[1]),\n",
    "    },\n",
    ")\n",
    "\n",
    "model = model.get_top_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor = Supervisor(\n",
    "    model=model, \n",
    "    data_train=features_train, \n",
    "    classes_train=classes_train, \n",
    "    filenames_train=filenames_train,\n",
    "    data_dev=features_dev, \n",
    "    classes_dev=classes_dev,\n",
    "    filenames_dev=filenames_dev,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor.train(\n",
    "    iterations=250000, \n",
    "    batch_size=128, \n",
    "    validation_size=350, \n",
    "    validate_every=5000, \n",
    "    learning_rate=0.00001,\n",
    "    k=[1,5,15,20],\n",
    "    #path='./siamese_model_weights/weights-55000-0.4714.hdf5',\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = supervisor.validate(\n",
    "    weights='../input/siamese_model_weights/weights-30000-0.8257.hdf5', \n",
    "    data=features_dev, \n",
    "    classes=classes_dev, \n",
    "    filenames=filenames_dev, \n",
    "    n=features_dev.shape[0], \n",
    "    batch_size=128, \n",
    "    ks=[1,5,15,20],\n",
    "    verbose=False,\n",
    ")\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
