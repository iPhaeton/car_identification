{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import img_size\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Flatten, BatchNormalization, subtract\n",
    "from keras.models import Model\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.regularizers import l2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seamese_Model:\n",
    "    def __init__(self, base_model, opts, kernel_opts=None, bias_opts=None):\n",
    "        self.base_model = base_model\n",
    "        self.opts = opts\n",
    "        self.kernel_opts = kernel_opts\n",
    "        self.bias_opts = bias_opts\n",
    "    \n",
    "    def kernel_initializer(self, shape, name=None):\n",
    "        if (self.kernel_opts != None):\n",
    "            values = np.random.normal(loc=self.kernel_opts['loc'], scale=self.kernel_opts['scale'], size=shape)\n",
    "            return K.variable(values, name=name)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def bias_initializer(self, shape, name=None):\n",
    "        if (self.bias_opts != None):\n",
    "            values = np.random.normal(loc=self.bias_opts['loc'], scale=self.bias_opts['scale'], size=shape)\n",
    "            return K.variable(values, name=name)\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def get_top_model(self):\n",
    "        input_1 = Input(self.opts['features_shape'])\n",
    "        input_2 = Input(self.opts['features_shape'])\n",
    "        \n",
    "        X_1 = Flatten()(input_1)\n",
    "        X_2 = Flatten()(input_2)\n",
    "        X = subtract([X_1, X_2])\n",
    "        X = Dense(self.opts['features_shape'][1], activation='relu', name='dense_0', kernel_regularizer=l2(1e-3))(X)\n",
    "        X = BatchNormalization()(X)\n",
    "        output = Dense(1, activation='sigmoid', name='log_reg', kernel_regularizer=l2(1e-3))(X)\n",
    "        \n",
    "        model = Model(input=[input_1, input_2], output=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Supervisor:\n",
    "    # use seed only for testing\n",
    "    def __init__(self, model, data_train, classes_train, filenames_train, data_dev, classes_dev, filenames_dev, seed=None):\n",
    "        self.model = model\n",
    "        self.data_train = data_train\n",
    "        self.classes_train = classes_train\n",
    "        self.filenames_train = filenames_train\n",
    "        self.data_dev = data_dev\n",
    "        self.classes_dev = classes_dev\n",
    "        self.filenames_dev = filenames_dev\n",
    "        self.seed = seed\n",
    "        \n",
    "    def get_pair(self, index_1, index_2):\n",
    "        el_1 = np.take(self.data_train, [index_1], axis=0)\n",
    "        el_2 = np.take(self.data_train, [index_2], axis=0)\n",
    "        return([el_1, el_2])\n",
    "    \n",
    "    def get_selection_index(self, index, indices):\n",
    "        selection_index = index\n",
    "        while selection_index == index:\n",
    "            selection_index = np.random.choice(indices, 1)[0]\n",
    "        return selection_index\n",
    "    \n",
    "    def get_batch(self, n, data, classes):\n",
    "        np.random.seed(self.seed)\n",
    "        indices = np.random.choice(list(range(len(data))), size=n)\n",
    "        \n",
    "        pairs = []\n",
    "        y = []\n",
    "        \n",
    "        for index in indices[:n//2]:\n",
    "            selection_indices = np.argwhere(classes == classes[index]).flatten()\n",
    "            selection_index = self.get_selection_index(index, selection_indices)\n",
    "            pairs.append(self.get_pair(index, selection_index))\n",
    "            y.append(1)\n",
    "            \n",
    "        for index in indices[n//2:]:\n",
    "            selection_indices = np.argwhere(classes != classes[index]).flatten()\n",
    "            selection_index = self.get_selection_index(index, selection_indices)\n",
    "            pairs.append(self.get_pair(index, selection_index))\n",
    "            y.append(0)\n",
    "\n",
    "        return (np.array(pairs), np.array(y))\n",
    "    \n",
    "    def train(self, iterations, batch_size, validation_size=0, validate_every=float('inf'), learning_rate=0.0001, path=None, rank=5):\n",
    "        self.model.compile(optimizer=tf.train.AdamOptimizer(learning_rate),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "        \n",
    "        if path != None:\n",
    "            self.model.load_weights(path)\n",
    "            \n",
    "        for i in tqdm_notebook(range(iterations)):\n",
    "            inputs, targets = self.get_batch(batch_size, self.data_train, self.classes_train)\n",
    "            metrics = self.model.train_on_batch([inputs[:,0,:], inputs[:,1,:]], targets)\n",
    "            if (i % validate_every == 0) & (i != 0):\n",
    "                self.checkpoint(batch_size, validation_size, i, metrics, rank)\n",
    "    \n",
    "    def get_validation_task(self, data, classes, filenames, show_filenames=False):\n",
    "        index = np.random.choice(range(data.shape[0]), 1)[0]\n",
    "        \n",
    "        targets = np.repeat(np.take(data, [index], axis=0), data.shape[0] - 1, axis=0)\n",
    "        support_set = np.delete(data, index, axis=0)\n",
    "        pairs = np.stack([targets, support_set], axis=1)\n",
    "        \n",
    "        pair_filenames = None\n",
    "        pair_classes = None\n",
    "        target_class = None\n",
    "        support_classes = None\n",
    "        if show_filenames == True:\n",
    "            target_filenames = np.repeat(np.take(filenames, [index], axis=0), data.shape[0] - 1, axis=0)\n",
    "            target_class = np.take(classes, [index], axis=0)\n",
    "            support_filenames = np.delete(filenames, index, axis=0)\n",
    "            support_classes = np.delete(classes, index, axis=0)\n",
    "            pair_filenames = np.stack([target_filenames, support_filenames], axis=1)\n",
    "        \n",
    "        target_y = classes[index]\n",
    "        y = (np.delete(classes, index) - target_y) == 0\n",
    "\n",
    "        return pairs.reshape(pairs.shape[0], pairs.shape[1], 1, pairs.shape[2]), y, pair_filenames, target_class, support_classes\n",
    "    \n",
    "    def calculate_accuracy(self, n, batch_size, data, classes, filenames, rank, verbose=False, show_filenames=False):\n",
    "        incorrect_answers_strict = 0\n",
    "        incorrect_answers_loose = 0\n",
    "        incorrect_filenames = []\n",
    "        correct_filenames = []\n",
    "        \n",
    "        for i in tqdm_notebook(range(n)):\n",
    "            inputs, targets, pair_filenames, target_class, support_classes = self.get_validation_task(data, classes, filenames, show_filenames)\n",
    "            predictions = self.model.predict([inputs[:,0,:], inputs[:,1,:]], batch_size=batch_size)\n",
    "\n",
    "            index = np.argmax(predictions.flatten())\n",
    "            indices = np.flipud(np.argsort(predictions.flatten()))[0:rank]\n",
    "            \n",
    "            if verbose == True:\n",
    "                max_value = np.max(predictions.flatten())\n",
    "                correct_indices = np.argwhere(targets == 1).flatten()\n",
    "                probs_for_correct_answers = np.take(predictions, correct_indices)\n",
    "                predicted_indices = np.argwhere(support_classes == support_classes[index])\n",
    "                probs_for_predicted_class = np.take(predictions, predicted_indices).flatten()\n",
    "                print('------------------------------------------------------')\n",
    "                print(max_value)\n",
    "                print('Correct:', probs_for_correct_answers)\n",
    "                print('Predicted:', probs_for_predicted_class)\n",
    "                print('------------------------------------------------------')\n",
    "            \n",
    "            if targets[index] != 1:\n",
    "                incorrect_answers_strict += 1\n",
    "                if show_filenames == True:\n",
    "                    incorrect_filenames.append(pair_filenames[index])\n",
    "            elif show_filenames == True:\n",
    "                correct_filenames.append(pair_filenames[index])    \n",
    "            \n",
    "            if np.sum(np.take(targets, indices)) == 0:\n",
    "                incorrect_answers_loose += 1\n",
    "                \n",
    "        return 1 - incorrect_answers_strict / n, 1 - incorrect_answers_loose / n, incorrect_filenames, correct_filenames\n",
    "    \n",
    "    def checkpoint(self, batch_size, validation_size, iteration, metrics, rank):\n",
    "        train_acc_strict, train_acc_loose, incorrect_filenames_train, correct_filenames_train = self.calculate_accuracy(\n",
    "            n=validation_size, \n",
    "            batch_size=batch_size, \n",
    "            data=self.data_train, \n",
    "            classes=self.classes_train, \n",
    "            filenames=self.filenames_train, \n",
    "            rank=rank,\n",
    "        )\n",
    "        dev_acc_strict, dev_acc_loose, incorrect_filenames_dev, correct_filenames_dev = self.calculate_accuracy(\n",
    "            n=validation_size, \n",
    "            batch_size=batch_size, \n",
    "            data=self.data_dev, \n",
    "            classes=self.classes_dev, \n",
    "            filenames=self.filenames_dev, \n",
    "            rank=rank,\n",
    "        )\n",
    "        print('Iteration ' + str(iteration) + '. Batch metrics [loss, accuracy]:', metrics)\n",
    "        print('Iteration ' + str(iteration) + '. Train accuracy strict:', train_acc_strict)\n",
    "        print('Iteration ' + str(iteration) + '. Validation accuracy strict:', dev_acc_strict)\n",
    "        print('Iteration ' + str(iteration) + '. Validation accuracy (rank=' + str(rank) + '):', dev_acc_loose)\n",
    "        print('---------')\n",
    "        self.model.save_weights('./model/weights-' + str(iteration) + '-' + str(round(dev_acc_strict, 4)) + '.hdf5')\n",
    "        \n",
    "    def validate(self, weights, data, classes, filenames, n=350, batch_size=128, rank=5, verbose=False):\n",
    "        self.model.load_weights(weights)\n",
    "        \n",
    "        acc_strict, acc_loose, incorrect_filenames, correct_filenames = self.calculate_accuracy(\n",
    "            n=n, \n",
    "            batch_size=batch_size, \n",
    "            data=data, \n",
    "            classes=classes, \n",
    "            filenames=filenames, \n",
    "            rank=rank,\n",
    "            show_filenames=True,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        \n",
    "        return acc_strict, acc_loose, incorrect_filenames, correct_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = np.load('./features/res_net/features_train.npy')\n",
    "classes_train = np.load('./features/res_net/classes_train.npy')\n",
    "filenames_train = np.load('./features/res_net/filenames_train.npy')\n",
    "features_dev = np.load('./features/res_net/features_dev.npy')\n",
    "classes_dev = np.load('./features/res_net/classes_dev.npy')\n",
    "filenames_dev = np.load('./features/res_net/filenames_dev.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:33: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"lo...)`\n"
     ]
    }
   ],
   "source": [
    "model = Seamese_Model(\n",
    "    ResNet50, \n",
    "    opts={\n",
    "        'weights': 'imagenet',\n",
    "        'input_shape': (img_size, img_size, 3),\n",
    "        'features_shape': (1, features_train.shape[1]),\n",
    "        'pooling': 'avg',\n",
    "    },\n",
    "    kernel_opts={\n",
    "        'loc': 0,\n",
    "        'scale': 1e-2,\n",
    "    },\n",
    "    bias_opts={\n",
    "        'loc': 0.5,\n",
    "        'scale': 1e-2,\n",
    "    }\n",
    ")\n",
    "\n",
    "model = model.get_top_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1, 2048)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1, 2048)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2048)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 2048)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, 2048)         0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 2048)         4196352     subtract_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 2048)         8192        dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "log_reg (Dense)                 (None, 1)            2049        batch_normalization_1[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 4,206,593\n",
      "Trainable params: 4,202,497\n",
      "Non-trainable params: 4,096\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor = Supervisor(\n",
    "    model=model, \n",
    "    data_train=features_train, \n",
    "    classes_train=classes_train, \n",
    "    filenames_train=filenames_train,\n",
    "    data_dev=features_dev, \n",
    "    classes_dev=classes_dev,\n",
    "    filenames_dev=filenames_dev,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd6ba51c4f04f0eb1ca6e008c063ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=250000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e125ea583d94d99b588269cc551033f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=350), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67926fe9306d488a8bb75078d0ec8a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=350), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10000. Batch metrics [loss, accuracy]: [0.53490615, 0.9296875]\n",
      "Iteration 10000. Train accuracy strict: 0.7114285714285714\n",
      "Iteration 10000. Validation accuracy strict: 0.2828571428571428\n",
      "Iteration 10000. Validation accuracy (rank=5): 0.5914285714285714\n",
      "---------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110efca263ef47948037bdabfa14dc70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=350), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e099ff617d64e8395ee1f886fe419b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=350), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20000. Batch metrics [loss, accuracy]: [0.52093422, 0.890625]\n",
      "Iteration 20000. Train accuracy strict: 0.6742857142857143\n",
      "Iteration 20000. Validation accuracy strict: 0.30000000000000004\n",
      "Iteration 20000. Validation accuracy (rank=5): 0.62\n",
      "---------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e4605015ec4c5ea2e85b2ebe46193a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=350), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f00946881c4440aad8be3288eb29efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=350), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30000. Batch metrics [loss, accuracy]: [0.56164736, 0.8828125]\n",
      "Iteration 30000. Train accuracy strict: 0.7285714285714286\n",
      "Iteration 30000. Validation accuracy strict: 0.32571428571428573\n",
      "Iteration 30000. Validation accuracy (rank=5): 0.6285714285714286\n",
      "---------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8156b4d81c7d45f3a6cbd5f2ddfdc3ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=350), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a3902395f7d4a7ab18c19fcc4ed08c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=350), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 40000. Batch metrics [loss, accuracy]: [0.57552886, 0.8828125]\n",
      "Iteration 40000. Train accuracy strict: 0.7114285714285714\n",
      "Iteration 40000. Validation accuracy strict: 0.29714285714285715\n",
      "Iteration 40000. Validation accuracy (rank=5): 0.6342857142857143\n",
      "---------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6368df33b786>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.000001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#    rank=5,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./model/weights-70000-0.2686.hdf5'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m  )\n",
      "\u001b[0;32m<ipython-input-3-70e2ca84004e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, iterations, batch_size, validation_size, validate_every, learning_rate, path, rank)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvalidate_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2719\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2720\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2721\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2693\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2694\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "supervisor.train(\n",
    "    iterations=250000, \n",
    "    batch_size=128, \n",
    "    validation_size=350, \n",
    "    validate_every=10000, \n",
    "    learning_rate=0.000001,\n",
    "#    rank=5,\n",
    "    path='./model/weights-70000-0.2686.hdf5',\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_strict, acc_loose, incorrect_filenames, correct_filenames = supervisor.validate(\n",
    "    weights='./model/batch_norm/weights-60000-0.18.hdf5', \n",
    "    data=features_dev, \n",
    "    classes=classes_dev, \n",
    "    filenames=filenames_dev, \n",
    "    n=350, \n",
    "    batch_size=128, \n",
    "    rank=5,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc_strict, acc_loose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    image_1 = np.array(ndimage.imread('./data/dev_dataset/' + incorrect_filenames[i][0], flatten=False))\n",
    "    image_2 = np.array(ndimage.imread('./data/dev_dataset/' + incorrect_filenames[i][1], flatten=False))\n",
    "    print(incorrect_filenames[i])\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    ax1 = fig.add_subplot(2,2,1)\n",
    "    ax1.imshow(image_1)\n",
    "    ax2 = fig.add_subplot(2,2,2)\n",
    "    ax2.imshow(image_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
